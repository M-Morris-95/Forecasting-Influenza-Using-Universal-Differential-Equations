{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from torchdiffeq import odeint\n",
    "from lib.models import Fp, Fa, FaFp, Encoder_z0_RNN\n",
    "import tqdm\n",
    "import lib.utils as utils\n",
    "from lib import train_functions as train_functions\n",
    "from lib.osthus_stuff import make_ics, reparam\n",
    "from torch.distributions import Normal, kl_divergence\n",
    "device = 'cpu'\n",
    "import torch \n",
    "torch.set_num_threads(1)\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib.Data_Constructor as Data_Constructor\n",
    "import lib.utils as utils\n",
    "import copy\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical, Normal, kl_divergence\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import chain\n",
    "import lib.utils as utils\n",
    "import lib.train_functions as train_functions\n",
    "from torchdiffeq import odeint\n",
    "import tqdm\n",
    "import lib.models as models\n",
    "torch.set_num_threads(1)\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 128\n",
    "latent_dim = 8\n",
    "hidden_dim = 64\n",
    "epochs = 1000\n",
    "lr=1e-3\n",
    "lr_scale = 0.999\n",
    "norm_weighting = 1e-1\n",
    "Fa_scale = 0.2\n",
    "disable = True\n",
    "\n",
    "ls = []\n",
    "for n_samples in [128]:\n",
    "    for latent_dim in [4,8,16]:\n",
    "        for hidden_dim in [16,32,64,128]:\n",
    "            for epochs in [2500, 6000]:\n",
    "                for lr in [1e-3, 5e-4, 1e-4]:\n",
    "                    for lr_scale in [0.999, 0.9999, 1.0]:\n",
    "                        for norm_weighting in [1.0, 1e-1, 1e-2, 1e-3]:\n",
    "                            for Fa_scale in [0.2]:\n",
    "                                ls.append({'n_samples':n_samples,'latent_dim':latent_dim, 'hidden_dim':hidden_dim, 'epochs':epochs,'lr':lr,'lr_scale':lr_scale, 'norm_weighting':norm_weighting, 'Fa_scale':Fa_scale})  \n",
    "\n",
    "\n",
    "df = pd.DataFrame.from_dict(ls)\n",
    "df['started'] = np.zeros(df.shape[0])\n",
    "df['validation_score'] = np.zeros(df.shape[0])\n",
    "\n",
    "df.index = np.linspace(0, df.shape[0]-1, df.shape[0]).astype(int)\n",
    "df.to_csv('validation_scores.csv')                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the model\n",
    "input_size = 11  # Number of features (time series)\n",
    "hidden_size = 32  # Number of features in the hidden state\n",
    "num_layers = 2  # Number of stacked LSTM layers\n",
    "latent_dim = 8  # Number of output classes\n",
    "n_samples = 64\n",
    "window_size = 28\n",
    "batch_size = 32\n",
    "root = 'checkpoints/SIR_Qs/'\n",
    "epochs = params['epochs']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(params, disable=True, quick=False):\n",
    "    input_size = int(params['input_size'])\n",
    "    hidden_size = int(params['hidden_size'])\n",
    "    num_layers = int(params['num_layers'])\n",
    "    latent_dim = int(params['latent_dim'])\n",
    "    n_samples = params['n_samples']\n",
    "    window_size = params['window_size']\n",
    "    batch_size = params['batch_size']\n",
    "    root = params['root']\n",
    "\n",
    "    \n",
    "    test_season = 2016\n",
    "    data_season = 2015\n",
    "    means=[0.8, 0.55]\n",
    "    stds = [0.2, 0.2]\n",
    "    lr = 1e-3\n",
    "    device='cpu'\n",
    "    gamma = 56\n",
    "    tmax = 56\n",
    "       \n",
    "\n",
    "\n",
    "    _data = Data_Constructor.DataConstructor(test_season, data_season, gamma, window_size, n_queries=input_size-1, selection_method='distance')\n",
    "    _data()\n",
    "    inputs, outputs, dates, test_dates = _data.build()\n",
    "    \n",
    "    test_start = np.where(dates == test_dates[0])[0][0]\n",
    "    dtype = torch.float32\n",
    "    x_tr = torch.tensor(inputs[:test_start], dtype=dtype)\n",
    "    x_test = torch.tensor(inputs[test_start:], dtype=dtype)\n",
    "    y_tr = torch.tensor(outputs[:test_start], dtype=dtype)\n",
    "    y_test = torch.tensor(outputs[test_start:], dtype=dtype)\n",
    "    \n",
    "    n_batches = int(np.ceil(x_tr.shape[0]/batch_size))\n",
    "    # batch it all \n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    for b in range(n_batches):\n",
    "        x_train.append(torch.tensor(x_tr[b*batch_size:(b+1)*batch_size], dtype=dtype))\n",
    "        y_train.append(torch.tensor(y_tr[b*batch_size:(b+1)*batch_size], dtype=dtype).unsqueeze(-2))\n",
    "    \n",
    "    # Create the model\n",
    "    n_regions = 1\n",
    "    enc = Encoder_BiDirectionalLSTM(input_size, hidden_size, num_layers, latent_dim-1)\n",
    "    ode = Fp(n_regions, latent_dim, nhidden=32)\n",
    "    dec = models.Decoder(n_regions, 3, 1)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(enc.parameters(), lr=1e-3)\n",
    "    pbar = tqdm.trange(30)\n",
    "    for epoch in pbar:\n",
    "        kls = 0\n",
    "        for x_tr in x_train:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            mean, std = enc(x_tr)\n",
    "            prior = make_prior(mean, latent_dim=latent_dim)\n",
    "            kl = kl_divergence(Normal(mean, std), prior).mean(0).sum()\n",
    "            kl.backward()\n",
    "            optimizer.step()\n",
    "            kls += kl.detach().numpy()\n",
    "            \n",
    "        pbar.set_postfix({'KL_z':kls})\n",
    "    \n",
    "    optimizer = torch.optim.Adam(chain(enc.parameters(), ode.parameters(), dec.parameters()), lr=1e-3)\n",
    "    _history = train_functions.history()\n",
    "    \n",
    "        \n",
    "    \n",
    "    epochs = 2000\n",
    "    pbar = tqdm.trange(epochs)\n",
    "    for epoch in pbar:\n",
    "        \n",
    "        t = torch.linspace(1, tmax, tmax)/7\n",
    "        epc = len(_history.epoch_history)\n",
    "        for x_tr, y_tr in zip(x_train, y_train):\n",
    "            batch_size = x_tr.shape[0]\n",
    "            eps = torch.tensor(np.random.normal(0, 1,  (n_samples, batch_size, n_regions, latent_dim-1)), dtype=dtype, device=device)\n",
    "            ode.clear_tracking()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            \n",
    "            mean, std = enc(x_tr)\n",
    "            z = reparam(eps, std, mean, n_samples, batch_size)\n",
    "            latent = odeint(ode, z, t, method='rk4', options=dict(step_size = 1.0))\n",
    "            y_pred = dec(latent[..., :3]).reshape((tmax, n_samples, batch_size, n_regions)).permute(2,1,0,3)\n",
    "            \n",
    "            nll = train_functions.nll_loss(y_pred, y_tr[:, :t.shape[0], :])\n",
    "            kl_p = train_functions.get_kl_params(1, ode.posterior(), means=means, stds = stds,limit = 500)\n",
    "            kl_z = kl_divergence(make_prior(mean, latent_dim=latent_dim), Normal(mean, std)).sum(-1).mean() / len(x_train)\n",
    "            reg_loss = train_functions.latent_init_loss(latent[..., :3])\n",
    "    \n",
    "            loss = nll+kl_p+kl_z+reg_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            _history.batch([loss.cpu(), nll.cpu(), kl_z.cpu(),kl_p.cpu(),reg_loss.cpu(), optimizer.param_groups[-1]['lr'], tmax], ['loss', 'nll', 'kl_latent', 'kl_params', 'reg_loss', 'lr', 'tmax'])\n",
    "        _history.reset()\n",
    "        if epoch > 10:\n",
    "            if np.all([h['nll'] < -2 for h in _history.epoch_history[-10:]]):\n",
    "                tmax = min(tmax+1, 28)\n",
    "        \n",
    "        utils.update_learning_rate(optimizer, 0.999, lr/10)\n",
    "        \n",
    "        if not os.path.exists(root):\n",
    "            os.mkdir(root)\n",
    "        torch.save(enc.state_dict(), root+'enc_' + '.pth')\n",
    "        torch.save(ode.state_dict(), root+'sir_' + '.pth')\n",
    "        torch.save(dec.state_dict(), root+'dec_' + '.pth')\n",
    "    \n",
    "        pbar.set_postfix(_history.epoch_history[-1])\n",
    "\n",
    "\n",
    "    mean, std = enc(torch.tensor(x_test))\n",
    "    batch_size = x_test.shape[0]\n",
    "    eps = torch.tensor(np.random.normal(0, 1,  (n_samples, batch_size, n_regions, latent_dim-1)), dtype=dtype, device=device)\n",
    "    z = reparam(eps, std, mean, n_samples, batch_size)\n",
    "    latent = odeint(ode, z, t, method='rk4', options=dict(step_size = 1.0))\n",
    "    y_pred = dec(latent[..., :3]).reshape((tmax, n_samples, batch_size, n_regions)).permute(2,1,0,3)*7.7151\n",
    "    \n",
    "    score = train_functions.nll_loss(y_pred, y_test.unsqueeze(-2)).detach().numpy()\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(params, disable=True, quick=False):\n",
    "    ili = pd.read_csv('Data/national_flu.csv', index_col = -1, parse_dates=True)\n",
    "\n",
    "    starts = ili[ili['week'] == 40].index\n",
    "    ends = [i + dt.timedelta(weeks=34) for i in starts]\n",
    "\n",
    "    starts = starts[[1,2,3,4,5,6,7,8,9,10,13,14,15,16,17,18]]\n",
    "    ends = np.asarray(ends)[[1,2,3,4,5,6,7,8,9,10,13,14,15,16,17,18]]\n",
    "\n",
    "    x_train = []\n",
    "    for start, end in zip(starts, ends):\n",
    "        x_train.append(torch.tensor(ili.loc[start:end, 'weighted_ili'].values/100, dtype=torch.float32))\n",
    "    \n",
    "    n_samples = int(params['n_samples'])\n",
    "    latent_dim = int(params['latent_dim'])\n",
    "    hidden_dim = int(params['hidden_dim'])\n",
    "    epochs = int(params['epochs'])\n",
    "    lr = params['lr']\n",
    "    lr_scale = params['lr_scale']\n",
    "    norm_weighting = params['norm_weighting']\n",
    "    Fa_scale = params['Fa_scale']\n",
    "    \n",
    "    if quick:\n",
    "        epochs=1\n",
    "    \n",
    "    dtype = torch.float32\n",
    "    t = torch.linspace(1,35,35, dtype=dtype)\n",
    "\n",
    "    ode = FaFp(latent_dim, hidden_dim, Fa_scale = Fa_scale)\n",
    "    enc = Encoder_z0_RNN(latent_dim, 1, Fp =True)\n",
    "\n",
    "    _history = train_functions.history()\n",
    "    optimizer = torch.optim.RAdam(list(enc.parameters()) + list(ode.parameters()), lr=lr)\n",
    "\n",
    "    ode.Fa_scale = Fa_scale\n",
    "\n",
    "\n",
    "    pbar = tqdm.trange(150, disable=disable)\n",
    "    for epoch in pbar:\n",
    "        for x in x_train[:-4]:        \n",
    "            IC_p = make_ics(x[0], I_0 = 0.88, std = 0.002, n = n_samples, latent=3)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            mean, std = enc(x.unsqueeze(0).unsqueeze(-1))\n",
    "\n",
    "            loss = 1e-4*kl_divergence(Normal(mean.squeeze()[..., :2], std.squeeze()[..., :2]), \n",
    "                                 Normal(IC_p.mean(0)[..., :2], IC_p.std(0)[..., :2])).sum()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_data = {'kl_z':loss.cpu()}\n",
    "            _history.batch(batch=batch_data)\n",
    "            utils.update_learning_rate(optimizer, 1.00, optimizer.param_groups[-1]['lr']/10)\n",
    "        _history.reset()\n",
    "        pbar.set_postfix(_history.epoch_history[-1])\n",
    "\n",
    "    pbar = tqdm.trange(epochs, disable=disable)\n",
    "    t = torch.linspace(1,35,35, dtype=dtype)\n",
    "    for epoch in pbar:\n",
    "        for x in x_train[:-4]:        \n",
    "            IC_p = make_ics(x[0], I_0 = 0.88, std = 0.002, n = n_samples, latent=3)\n",
    "            z = torch.tensor(np.random.normal(0, 1, (n_samples, latent_dim-1)), dtype=dtype, device=device)\n",
    "\n",
    "            ode.clear_tracking()\n",
    "            optimizer.zero_grad()\n",
    "            mean, std = enc(x[:5].unsqueeze(0).unsqueeze(-1))\n",
    "            IC = reparam(mean, std, z)\n",
    "\n",
    "            latent = odeint(ode, IC, t, method='rk4', options=dict(step_size = 1.0))\n",
    "\n",
    "            y_mean = latent.mean(1)[...,1]\n",
    "            y_std = latent.std(1)[...,1]\n",
    "\n",
    "            nll = torch.tensor(0)\n",
    "            if len(_history.epoch_history) > 200:\n",
    "                nll = torch.mean(-Normal(y_mean, y_std).log_prob(x))\n",
    "\n",
    "            kl_p = kl_divergence(Normal(torch.stack(ode.params).mean(1), torch.stack(ode.params).std(1, unbiased=False)), \n",
    "                                 Normal(torch.tensor([0.8, 0.55]), torch.tensor([0.25, 0.25]))).mean()\n",
    "            kl_z = 1e-1*kl_divergence(Normal(mean.squeeze()[..., :2], std.squeeze()[..., :2]), \n",
    "                                 Normal(IC_p.mean(0)[..., :2], IC_p.std(0)[..., :2])).sum()\n",
    "            norm_loss = norm_weighting*torch.sqrt(torch.sum(torch.square(torch.stack(ode.Fa_track)[..., :2])))\n",
    "\n",
    "            loss = 10*nll + kl_p + kl_z + norm_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_data = {'Loss':loss.cpu(), 'nll':nll.cpu(), 'kl_p':kl_p.cpu(), 'kl_z':kl_z.cpu(),'norm_loss':norm_loss.cpu(), 'lr':optimizer.param_groups[-1]['lr']}\n",
    "            _history.batch(batch=batch_data)\n",
    "            utils.update_learning_rate(optimizer, lr_scale, optimizer.param_groups[-1]['lr']/10)\n",
    "        _history.reset()\n",
    "        pbar.set_postfix(_history.epoch_history[-1])\n",
    "\n",
    "    score = 0\n",
    "    for x in x_train[-4:]:        \n",
    "        z = torch.tensor(np.random.normal(0, 1, (n_samples, latent_dim-1)), dtype=dtype, device=device)\n",
    "\n",
    "        ode.clear_tracking()\n",
    "        mean, std = enc(x[:5].unsqueeze(0).unsqueeze(-1))\n",
    "        IC = reparam(mean, std, z)\n",
    "\n",
    "        latent = odeint(ode, IC, t, method='rk4', options=dict(step_size = 1.0))\n",
    "\n",
    "        y_mean = latent.mean(1)[...,1]\n",
    "        y_std = latent.std(1)[...,1]\n",
    "\n",
    "        nll = torch.mean(-Normal(y_mean, y_std).log_prob(x))\n",
    "        score += float(nll.detach())\n",
    "        \n",
    "    return score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:12<00:00, 12.30it/s, kl_z=9.12e-5] \n",
      "100%|██████████| 55/55 [01:18<00:00,  1.43s/it, Loss=362, nll=3.61, kl_p=2.06, kl_z=324, norm_loss=0.0129, lr=0.00052]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.213887482881546\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "from filelock import FileLock\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "position = 1\n",
    "\n",
    "\n",
    "lock = FileLock(\"validation_scores.csv.lock\")\n",
    "\n",
    "with lock:\n",
    "    df = pd.read_csv('validation_scores.csv', index_col=0)\n",
    "    try:\n",
    "        param_num = np.min(np.where(df['started'] == 0))\n",
    "    except:\n",
    "        print('oh no')\n",
    "    df.loc[param_num, 'started'] = 1\n",
    "    df.to_csv('validation_scores.csv')\n",
    "\n",
    "score = 10\n",
    "try:\n",
    "    score = evaluate(dict(df.loc[param_num]), disable=False)\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n",
    "\n",
    "\n",
    "with lock:\n",
    "    df = pd.read_csv('validation_scores.csv', index_col=0)\n",
    "    df.loc[param_num, 'score'] = score\n",
    "    df.to_csv('validation_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(params, disable=disable):\n",
    "    ili = pd.read_csv('Data/national_flu.csv', index_col = -1, parse_dates=True)\n",
    "\n",
    "    starts = ili[ili['week'] == 40].index\n",
    "    ends = [i + dt.timedelta(weeks=34) for i in starts]\n",
    "\n",
    "    starts = starts[[1,2,3,4,5,6,7,8,9,10,13,14,15,16,17,18]]\n",
    "    ends = np.asarray(ends)[[1,2,3,4,5,6,7,8,9,10,13,14,15,16,17,18]]\n",
    "\n",
    "    x_train = []\n",
    "    for start, end in zip(starts, ends):\n",
    "        x_train.append(torch.tensor(ili.loc[start:end, 'weighted_ili'].values/100, dtype=torch.float32))\n",
    "    \n",
    "    n_samples = params['n_samples']\n",
    "    latent_dim = params['latent_dim']\n",
    "    hidden_dim = params['hidden_dim']\n",
    "    epochs = params['epochs']\n",
    "    lr = params['lr']\n",
    "    n_samples = params['n_samples']\n",
    "    lr_scale = params['lr_scale']\n",
    "    norm_weighting = params['norm_weighting']\n",
    "    Fa_scale = params['Fa_scale']\n",
    "    \n",
    "    dtype = torch.float32\n",
    "    t = torch.linspace(1,35,35, dtype=dtype)\n",
    "\n",
    "    ode = FaFp(latent_dim, hidden_dim, Fa_scale = Fa_scale)\n",
    "    enc = Encoder_z0_RNN(latent_dim, 1, Fp =True)\n",
    "\n",
    "    _history = train_functions.history()\n",
    "    optimizer = torch.optim.RAdam(list(enc.parameters()) + list(ode.parameters()), lr=lr)\n",
    "\n",
    "    ode.Fa_scale = Fa_scale\n",
    "\n",
    "    pbar = tqdm.trange(150, disable=disable)\n",
    "    for epoch in pbar:\n",
    "        for x in x_train[:-4]:        \n",
    "            IC_p = make_ics(x[0], I_0 = 0.88, std = 0.002, n = n_samples, latent=3)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            mean, std = enc(x.unsqueeze(0).unsqueeze(-1))\n",
    "\n",
    "            loss = 1e-4*kl_divergence(Normal(mean.squeeze()[..., :2], std.squeeze()[..., :2]), \n",
    "                                 Normal(IC_p.mean(0)[..., :2], IC_p.std(0)[..., :2])).sum()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_data = {'kl_z':loss.cpu()}\n",
    "            _history.batch(batch=batch_data)\n",
    "            utils.update_learning_rate(optimizer, 1.00, optimizer.param_groups[-1]['lr']/10)\n",
    "        _history.reset()\n",
    "        pbar.set_postfix(_history.epoch_history[-1])\n",
    "\n",
    "    pbar = tqdm.trange(epochs, disable=disable)\n",
    "    t = torch.linspace(1,35,35, dtype=dtype)\n",
    "    for epoch in pbar:\n",
    "        for x in x_train[:-4]:        \n",
    "            IC_p = make_ics(x[0], I_0 = 0.88, std = 0.002, n = n_samples, latent=3)\n",
    "            z = torch.tensor(np.random.normal(0, 1, (n_samples, 7)), dtype=dtype, device=device)\n",
    "\n",
    "            ode.clear_tracking()\n",
    "            optimizer.zero_grad()\n",
    "            mean, std = enc(x[:5].unsqueeze(0).unsqueeze(-1))\n",
    "            IC = reparam(mean, std, z)\n",
    "\n",
    "            latent = odeint(ode, IC, t, method='rk4', options=dict(step_size = 1.0))\n",
    "\n",
    "            y_mean = latent.mean(1)[...,1]\n",
    "            y_std = latent.std(1)[...,1]\n",
    "\n",
    "            nll = torch.tensor(0)\n",
    "            if len(_history.epoch_history) > 200:\n",
    "                nll = torch.mean(-Normal(y_mean, y_std).log_prob(x))\n",
    "\n",
    "            kl_p = kl_divergence(Normal(torch.stack(ode.params).mean(1), torch.stack(ode.params).std(1, unbiased=False)), \n",
    "                                 Normal(torch.tensor([0.8, 0.55]), torch.tensor([0.25, 0.25]))).mean()\n",
    "            kl_z = 1e-1*kl_divergence(Normal(mean.squeeze()[..., :2], std.squeeze()[..., :2]), \n",
    "                                 Normal(IC_p.mean(0)[..., :2], IC_p.std(0)[..., :2])).sum()\n",
    "            norm_loss = norm_weighting*torch.sqrt(torch.sum(torch.square(torch.stack(ode.Fa_track)[..., :2])))\n",
    "\n",
    "            loss = 10*nll + kl_p + kl_z + norm_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_data = {'Loss':loss.cpu(), 'nll':nll.cpu(), 'kl_p':kl_p.cpu(), 'kl_z':kl_z.cpu(),'norm_loss':norm_loss.cpu(), 'lr':optimizer.param_groups[-1]['lr']}\n",
    "            _history.batch(batch=batch_data)\n",
    "            utils.update_learning_rate(optimizer, lr_scale, optimizer.param_groups[-1]['lr']/10)\n",
    "        _history.reset()\n",
    "        pbar.set_postfix(_history.epoch_history[-1])\n",
    "\n",
    "    score = 0\n",
    "    for x in x_train[-4:]:        \n",
    "        z = torch.tensor(np.random.normal(0, 1, (n_samples, 7)), dtype=dtype, device=device)\n",
    "\n",
    "        ode.clear_tracking()\n",
    "        mean, std = enc(x[:5].unsqueeze(0).unsqueeze(-1))\n",
    "        IC = reparam(mean, std, z)\n",
    "\n",
    "        latent = odeint(ode, IC, t, method='rk4', options=dict(step_size = 1.0))\n",
    "\n",
    "        y_mean = latent.mean(1)[...,1]\n",
    "        y_std = latent.std(1)[...,1]\n",
    "\n",
    "        nll = torch.mean(-Normal(y_mean, y_std).log_prob(x))\n",
    "        score += float(nll.detach())\n",
    "        \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot((y_mean+y_std).detach())\n",
    "plt.plot((y_mean-y_std).detach())\n",
    "plt.plot((y_mean).detach())\n",
    "plt.plot(x, color='red')\n",
    "print(float(torch.mean(-Normal(y_mean, y_std).log_prob(x)).detach()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
