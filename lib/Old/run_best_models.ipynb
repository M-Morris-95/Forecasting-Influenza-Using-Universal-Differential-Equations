{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8afc1327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import datetime as dt\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "import argparse\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "from scipy import interpolate\n",
    "from filelock import FileLock\n",
    "from itertools import chain\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from torchdiffeq import odeint\n",
    "from torch.distributions import Categorical, Normal, kl_divergence\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import random\n",
    "import ast\n",
    "\n",
    "# PyTorch related imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Utilities and custom modules\n",
    "import lib.utils as utils\n",
    "import lib.models as models\n",
    "import lib.train_functions as train_functions\n",
    "import lib.encoders as encoders\n",
    "from lib.HHS_data import *\n",
    "\n",
    "# Setting the number of threads for PyTorch and specifying the device\n",
    "torch.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3b23705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(params, year, save_file, suppress_outputs = True):\n",
    "    def convert_data(inputs, outputs, year=2015, batch_size=32):\n",
    "        train_dates = {\n",
    "            2015: {'start': dt.date(2004, 3, 24), 'end': dt.date(2015, 8, 12)},\n",
    "            2016: {'start': dt.date(2004, 3, 24), 'end': dt.date(2016, 8, 11)},\n",
    "            2017: {'start': dt.date(2004, 3, 24), 'end': dt.date(2017, 8, 10)},\n",
    "            2018: {'start': dt.date(2004, 3, 24), 'end': dt.date(2018, 8, 9)}\n",
    "        }\n",
    "\n",
    "        test_dates = {\n",
    "            2015: {'start': dt.date(2015, 10, 19), 'end': dt.date(2016, 5, 14)},\n",
    "            2016: {'start': dt.date(2016, 10, 17), 'end': dt.date(2017, 5, 13)},\n",
    "            2017: {'start': dt.date(2017, 10, 16), 'end': dt.date(2018, 5, 12)},\n",
    "            2018: {'start': dt.date(2018, 10, 15), 'end': dt.date(2019, 5, 11)}\n",
    "        }\n",
    "\n",
    "        tr_start_idx = np.where([train_dates[year]['start'] == d for d in dates])[0][0]\n",
    "        tr_end_idx = np.where([train_dates[year]['end'] == d for d in dates])[0][0]\n",
    "        te_start_idx = np.where([test_dates[year]['start'] == d for d in dates])[0][0]\n",
    "        te_end_idx = np.where([test_dates[year]['end'] == d for d in dates])[0][0]\n",
    "\n",
    "        x_tr, y_tr = inputs[tr_start_idx:tr_end_idx], outputs[tr_start_idx:tr_end_idx]\n",
    "        x_test, y_test = inputs[te_start_idx:te_end_idx], outputs[te_start_idx:te_end_idx]\n",
    "\n",
    "        x_train = [x_tr[b * batch_size:(b + 1) * batch_size] for b in range(int(np.ceil(x_tr.shape[0] / batch_size)))]\n",
    "        y_train = [y_tr[b * batch_size:(b + 1) * batch_size] for b in range(int(np.ceil(y_tr.shape[0] / batch_size)))]\n",
    "\n",
    "        return x_train, y_train, x_test, y_test\n",
    "\n",
    "    window = 35\n",
    "    year = 2015\n",
    "    lag = 14\n",
    "    n_regions = 10\n",
    "    season = 2016\n",
    "    lr = 1e-3\n",
    "    n_samples = 128\n",
    "    device = 'cpu'\n",
    "    torch.set_num_threads(1)\n",
    "    dtype=torch.float32\n",
    "\n",
    "    n_qs = int(params['n_qs'])\n",
    "    latent_dim = int(params['latent_dim'])\n",
    "    means = ast.literal_eval(params['means'])\n",
    "    stds = ast.literal_eval(params['stds'])\n",
    "    q_sizes = ast.literal_eval(params['q_sizes'])\n",
    "    ff_sizes = ast.literal_eval(params['ff_sizes'])\n",
    "    ili_sizes = ast.literal_eval(params['ili_sizes'])\n",
    "    SIR_scaler = ast.literal_eval(params['SIR_scaler'])\n",
    "    anneal = int(params['anneal'])\n",
    "    epochs = int(params['epochs'])\n",
    "    enc_name = params['enc_name']\n",
    "\n",
    "    if enc_name == 'Encoder_BiDirectionalGRU':\n",
    "        encoder_model = encoders.Encoder_BiDirectionalGRU\n",
    "        run_backward = False\n",
    "    if enc_name == 'Encoder_MISO_GRU':\n",
    "        encoder_model = encoders.Encoder_MISO_GRU\n",
    "        run_backward = False\n",
    "    if enc_name == 'Encoder_Back_GRU':\n",
    "        encoder_model = encoders.Encoder_Back_GRU\n",
    "        run_backward = True\n",
    "\n",
    "    if encoder_model == encoders.Encoder_Back_GRU:   \n",
    "        gamma = 28\n",
    "    else:\n",
    "        gamma = 63\n",
    "\n",
    "    ili = load_ili('hhs')\n",
    "    ili = intepolate_ili(ili)\n",
    "\n",
    "    hhs_dict = {}\n",
    "    qs_dict = {}\n",
    "\n",
    "    ignore = ['VI', 'PR']\n",
    "    for i in range(1,1+n_regions):\n",
    "        hhs_dict[i] = get_hhs_query_data(i, ignore=ignore, smooth_after = True)\n",
    "        qs_dict[i] = choose_qs(hhs_dict, ili, i, season, n_qs)\n",
    "\n",
    "        hhs_dict[i] = hhs_dict[i].loc[:, list(qs_dict[i])]\n",
    "        hhs_dict[i] = hhs_dict[i].div(hhs_dict[i].max())\n",
    "        \n",
    "    ili = ili.loc[hhs_dict[i].index[0] : hhs_dict[i].index[-1]]\n",
    "    ili = ili.div(ili.max())\n",
    "\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    dates = []\n",
    "    for batch in range(ili.shape[0] - (window+gamma)):\n",
    "        batch_inputs = []\n",
    "        for i in range(1,11):\n",
    "            batch_inputs.append(hhs_dict[i].iloc[batch:batch+window+1])\n",
    "        \n",
    "        t_ili = ili.iloc[batch:batch+window+1].copy()\n",
    "        t_ili.iloc[-lag:, :] = -1\n",
    "        dates.append(pd.to_datetime(t_ili.index[-15]).date())\n",
    "        batch_inputs.append(t_ili)\n",
    "        batch_inputs = np.concatenate(batch_inputs, -1)\n",
    "\n",
    "        if run_backward:\n",
    "            gamma = 28\n",
    "            batch_outputs = ili.iloc[batch:batch+window-lag+gamma+1].values\n",
    "            t = torch.linspace(0, batch_outputs.shape[0]-1, batch_outputs.shape[0])/7\n",
    "        else:\n",
    "            gamma = 56\n",
    "            batch_outputs = ili.iloc[batch+window-lag:batch+window-lag+gamma+1].values\n",
    "            t = torch.linspace(0, batch_outputs.shape[0]-1, batch_outputs.shape[0])/7\n",
    "            \n",
    "        inputs.append(batch_inputs)\n",
    "        outputs.append(batch_outputs)\n",
    "    inputs = torch.tensor(np.asarray(inputs), dtype=torch.float32)\n",
    "    outputs = torch.tensor(np.asarray(outputs), dtype=torch.float32)\n",
    "\n",
    "\n",
    "    batch_size = 32\n",
    "    new_inputs = torch.tensor(np.asarray(inputs), dtype=torch.float32).to(device)\n",
    "    new_outputs = torch.tensor(np.asarray(outputs), dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "    x_train, y_train, x_test, y_test = convert_data(new_inputs, new_outputs, year = year, batch_size = 32)\n",
    "\n",
    "    enc = encoder_model(n_regions, \n",
    "                    n_qs=n_qs,\n",
    "                    latent_dim = latent_dim-1,    \n",
    "                    q_sizes=q_sizes, \n",
    "                    ili_sizes=ili_sizes, \n",
    "                    ff_sizes = ff_sizes, \n",
    "                    SIR_scaler = SIR_scaler, \n",
    "                    device=device, \n",
    "                    dtype=torch.float32)\n",
    "\n",
    "    ode = models.Fp(n_regions, latent_dim, nhidden=64)\n",
    "    dec = models.Decoder(n_regions, 3, 1, device=device)\n",
    "\n",
    "    enc.to(device)\n",
    "    ode.to(device)\n",
    "    dec.to(device)\n",
    "\n",
    "    if not suppress_outputs:\n",
    "        num = np.sum([np.prod(_.shape) for _ in list(enc.parameters())])\n",
    "        print('encoder parameters:', num)\n",
    "        \n",
    "        num = np.sum([np.prod(_.shape) for _ in list(ode.parameters())])\n",
    "        print('ode parameters:', num)\n",
    "        \n",
    "        num = np.sum([np.prod(_.shape) for _ in list(dec.parameters())])\n",
    "        print('decoder parameters:', num)\n",
    "        \n",
    "    # pre train\n",
    "    print('pre training')\n",
    "    optimizer = torch.optim.Adam(enc.parameters(), lr=lr)\n",
    "    for epoch in range(3):\n",
    "        kls = 0\n",
    "        pbar = tqdm.tqdm(x_train, disable=suppress_outputs)\n",
    "        num = 0\n",
    "        for x_tr in pbar:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            mean, std = enc(x_tr)\n",
    "            prior = encoders.make_prior(mean, latent_dim=latent_dim, device=device)\n",
    "            kl = kl_divergence(Normal(mean, std), prior).mean(0).sum()\n",
    "            if torch.isnan(kl):\n",
    "                break\n",
    "            kl.backward()\n",
    "            optimizer.step()\n",
    "            kls += kl.cpu().detach().numpy()\n",
    "            num += 1\n",
    "            pbar.set_postfix({'Epoch':epoch, 'KL_z':kls/num})\n",
    "            \n",
    "    optimizer = torch.optim.Adam(chain(enc.parameters(), ode.parameters(), dec.parameters()), lr=lr)\n",
    "    _history = train_functions.history()\n",
    "    n_samples = 64\n",
    "    kl_w = 1\n",
    "    step = 0\n",
    "\n",
    "    track_norms = []\n",
    "    print('training')\n",
    "    eval_pts = np.arange(0,t.shape[-1], 7)\n",
    "    for epoch in range(epochs):\n",
    "        batch_grad_norms = []\n",
    "        pbar = tqdm.tqdm(zip(x_train, y_train), disable=suppress_outputs)\n",
    "        for x_tr, y_tr in pbar:\n",
    "            batch_size = x_tr.shape[0]\n",
    "            if anneal:\n",
    "                step += 1 \n",
    "                kl_w = train_functions.KL_annealing(step, reset_pos=10000, split=0.5, lower = 0.0, upper = 1.0, type = 'cosine')\n",
    "            eps = torch.randn(n_samples, batch_size, n_regions, latent_dim-1, dtype=dtype, device=device)\n",
    "            ode.clear_tracking()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            mean, std = enc(x_tr)\n",
    "            z = encoders.reparam(eps, std, mean, n_samples, batch_size)\n",
    "            latent = odeint(ode, z, t[eval_pts], method='rk4', options=dict(step_size = 1.0))\n",
    "            y_pred = dec(latent[..., :3]).reshape((-1, n_samples, batch_size, n_regions)).permute(2,1,0,3)\n",
    "\n",
    "            nll = train_functions.nll_loss(y_pred, y_tr[:, eval_pts, :])\n",
    "            kl_p = train_functions.get_kl_params(1, ode.posterior(), means=means, stds = stds,limit = 1e6, device=device)\n",
    "            kl_z = kl_w*kl_divergence(encoders.make_prior(mean, latent_dim=latent_dim, device=device), Normal(mean, std)).sum(-1).mean() / len(x_train)\n",
    "            reg_loss = train_functions.latent_init_loss(latent[..., :3])\n",
    "\n",
    "            loss = nll+kl_p+kl_z+reg_loss\n",
    "            loss.backward()\n",
    "\n",
    "\n",
    "            # Check gradient magnitudes\n",
    "            grad_norm = torch.norm(torch.cat([p.grad.data.view(-1) for p in chain(enc.parameters(), ode.parameters(), dec.parameters())]), 2).item()\n",
    "            batch_grad_norms.append(grad_norm)\n",
    "\n",
    "            gradient_threshold = 300\n",
    "            if grad_norm < gradient_threshold or epoch <= 3:\n",
    "                optimizer.step()\n",
    "\n",
    "            # _history.batch([loss.cpu(), nll.cpu(), kl_z.cpu(),kl_p.cpu(),reg_loss.cpu(), optimizer.param_groups[-1]['lr'], kl_w], ['loss', 'nll', 'kl_latent', 'kl_params', 'reg_loss', 'lr', 'kl_w'])\n",
    "            _history.batch([round(loss.cpu().item(), 3), \n",
    "                            round(nll.cpu().item(), 3), \n",
    "                            round(kl_z.cpu().item(), 3), \n",
    "                            round(kl_p.cpu().item(), 3), \n",
    "                            round(reg_loss.cpu().item(), 3), \n",
    "                            round(optimizer.param_groups[-1]['lr'], 3), \n",
    "                            round(kl_w, 3), ], \n",
    "                            ['loss', 'nll', 'kl_latent', 'kl_params', 'reg_loss', 'lr', 'kl_w'])\n",
    "\n",
    "            pbar.set_postfix(_history.epoch())\n",
    "        _history.reset()\n",
    "\n",
    "        if not suppress_outputs:\n",
    "            with open('grad_norms.txt', 'a') as file:\n",
    "                file.write(','.join(map(str, batch_grad_norms)) + '\\n')\n",
    "\n",
    "        rounded_epoch_history = {key: round(value, 3) for key, value in _history.epoch_history[-1].items()}\n",
    "        rounded_epoch_history['grad_norm'] = max(batch_grad_norms)\n",
    "        if suppress_outputs:\n",
    "            print(epoch + 1, dt.datetime.now().strftime('%Y-%m-%d %H:%M:%S'), rounded_epoch_history)\n",
    "        utils.update_learning_rate(optimizer, 0.999, lr/10)\n",
    "\n",
    "    n_samples = 512\n",
    "    batch_size = x_test.shape[0]\n",
    "    eps = torch.randn(n_samples, batch_size, n_regions, latent_dim-1, dtype=dtype, device=device)\n",
    "    ode.clear_tracking()\n",
    "    mean, std = enc(x_test)\n",
    "    z = encoders.reparam(eps, std, mean, n_samples, batch_size)\n",
    "    latent = odeint(ode, z, t, method='rk4', options=dict(step_size = 1.0))\n",
    "    y_pred = dec(latent[..., :3]).reshape((t.shape[0], n_samples, batch_size, n_regions)).permute(2,1,0,3)\n",
    "    pred_mean = y_pred.mean(1)\n",
    "    pred_std = y_pred.std(1)\n",
    "    nll = train_functions.nll_loss(y_pred, y_test).detach().cpu().numpy()\n",
    "\n",
    "    results = {'ground_truth':y_test.numpy().tolist(),\n",
    "            'mean':pred_mean.detach().numpy().tolist(),\n",
    "            'std': pred_std.detach().numpy().tolist(),\n",
    "            'nll':nll.item()}\n",
    "\n",
    "\n",
    "    with open(save_file, 'w') as json_file:\n",
    "        json.dump(results, json_file)\n",
    "\n",
    "    return nll.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ff7e2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('validation_scores.csv', index_col=0)\n",
    "\n",
    "test_scores = pd.DataFrame(index = df.index, columns = ['runs 2015','runs 2016','runs 2017','runs 2018', 'score 2015', 'score 2016', 'score 2017', 'score 2018'], data=0)\n",
    "test_scores.to_csv('test_scores.csv')\n",
    "df = df.sort_values('score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9ad16dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('validation_scores.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f0a26ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_47616/3312892505.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['started'][[np.isnan(d) for d in df['score']]] = 0\n"
     ]
    }
   ],
   "source": [
    "# df['started'][[np.isnan(d) for d in df['score']]] = 0\n",
    "# df.to_csv('validation_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9ca4158a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/209/save_data_3-2018.json\n",
      "results/209/save_data_4-2015.json\n",
      "results/209/save_data_4-2016.json\n",
      "results/209/save_data_4-2017.json\n",
      "results/209/save_data_4-2018.json\n",
      "results/209/save_data_5-2015.json\n",
      "results/209/save_data_5-2016.json\n",
      "results/209/save_data_5-2017.json\n",
      "results/209/save_data_5-2018.json\n",
      "results/395/save_data_1-2015.json\n",
      "results/395/save_data_1-2016.json\n",
      "results/395/save_data_1-2017.json\n",
      "results/395/save_data_1-2018.json\n",
      "results/395/save_data_2-2015.json\n",
      "results/395/save_data_2-2016.json\n",
      "results/395/save_data_2-2017.json\n",
      "results/395/save_data_2-2018.json\n",
      "results/395/save_data_3-2015.json\n",
      "results/395/save_data_3-2016.json\n",
      "results/395/save_data_3-2017.json\n",
      "results/395/save_data_3-2018.json\n",
      "results/395/save_data_4-2015.json\n",
      "results/395/save_data_4-2016.json\n",
      "results/395/save_data_4-2017.json\n",
      "results/395/save_data_4-2018.json\n",
      "results/395/save_data_5-2015.json\n",
      "results/395/save_data_5-2016.json\n",
      "results/395/save_data_5-2017.json\n",
      "results/395/save_data_5-2018.json\n",
      "results/95/save_data_1-2015.json\n",
      "results/95/save_data_1-2016.json\n",
      "results/95/save_data_1-2017.json\n",
      "results/95/save_data_1-2018.json\n",
      "results/95/save_data_2-2015.json\n",
      "results/95/save_data_2-2016.json\n",
      "results/95/save_data_2-2017.json\n",
      "results/95/save_data_2-2018.json\n",
      "results/95/save_data_3-2015.json\n",
      "results/95/save_data_3-2016.json\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[91], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m     save_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(df\u001b[38;5;241m.\u001b[39miloc[num]\u001b[38;5;241m.\u001b[39mname) \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/save_data_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(run)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(year)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(save_file)\n\u001b[1;32m---> 27\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     29\u001b[0m     num \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from filelock import FileLock\n",
    "\n",
    "num = 0\n",
    "suppress_outputs = True\n",
    "\n",
    "# Use FileLock for reading and modifying the DataFrame\n",
    "with FileLock('validation_scores.csv.lock'):\n",
    "    df = pd.read_csv('validation_scores.csv', index_col=0)\n",
    "    df = df.sort_values('score')\n",
    "\n",
    "while num < 10:\n",
    "    # Use FileLock for reading and modifying test scores\n",
    "    with FileLock('test_scores.csv.lock'):\n",
    "        test_scores = pd.read_csv('test_scores.csv', index_col=0)\n",
    "        idx = df.iloc[num].name\n",
    "        params = df.iloc[num]\n",
    "\n",
    "        scores = test_scores.iloc[idx]\n",
    "\n",
    "        min_runs = np.min(scores[['runs' in s for s in scores.index]])\n",
    "\n",
    "        if min_runs < 5:\n",
    "            pos = np.argmin(scores[['runs' in s for s in scores.index]])\n",
    "            year = int(scores.index[pos].split(' ')[1])\n",
    "            run = min_runs + 1\n",
    "\n",
    "            test_scores.iloc[idx, pos] = run\n",
    "\n",
    "            # Use FileLock for writing to test_scores.csv\n",
    "            with FileLock('test_scores.csv.lock'):\n",
    "                test_scores.to_csv('test_scores.csv')\n",
    "\n",
    "            result_folder = 'results/' + str(df.iloc[num].name)\n",
    "            if not os.path.exists(result_folder):\n",
    "                os.mkdir(result_folder)\n",
    "\n",
    "            save_file = os.path.join(result_folder, f'save_data_{run}-{year}.json')\n",
    "            \n",
    "            print(save_file)\n",
    "            \n",
    "            params['epochs'] = 1\n",
    "            nll = test(params, year, save_file, suppress_outputs = True)\n",
    "            print(nll)\n",
    "            time.sleep(0.1)\n",
    "\n",
    "            \n",
    "        else:\n",
    "            num += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d74949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
