{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy import interpolate\n",
    "from scipy.stats import norm\n",
    "import torch\n",
    "from itertools import chain\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from lib.encoder_back_gru import Encoder_Back_GRU\n",
    "import lib.Metrics as Metrics\n",
    "from lib.hhs_data_builder import DataConstructor, convert_to_torch\n",
    "# from lib.encoders import \n",
    "from torch import nn\n",
    "import lib.encoders as encoders\n",
    "from torchdiffeq import odeint\n",
    "import lib.train_functions as train_functions\n",
    "import lib.utils as utils\n",
    "from lib.UONN import FaFp\n",
    "from lib.models import Decoder, Fa, Fp\n",
    "from torch.distributions import Categorical, Normal\n",
    "from lib.VAE import VAE\n",
    "from filelock import FileLock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_num_threads(1)\n",
    "\n",
    "batch_size = 32\n",
    "window_size = 28\n",
    "n_regions = 49\n",
    "lr = 1e-3\n",
    "dtype = torch.float32\n",
    "gamma = 28\n",
    "\n",
    "\n",
    "t = torch.arange(window_size + gamma + 1, dtype=dtype)/7\n",
    "eval_pts = np.arange(0,t.shape[-1], 7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "window_size = 28\n",
    "n_regions = 49\n",
    "n_qs = 9\n",
    "latent_dim = 8\n",
    "lr = 1e-3\n",
    "dtype = torch.float32\n",
    "gamma = 28\n",
    "n_samples = 64\n",
    "\n",
    "mydict =   {'Full_FaFp':['UONN', {'nll':True, 'kl_z':True, 'kl_p':True,  'Fa_norm':True,  'reg_loss':True,  'anneal':True}],\n",
    "            'Half_FaFp':['UONN', {'nll':True, 'kl_z':True, 'kl_p':False, 'Fa_norm':True,  'reg_loss':False, 'anneal':True}],\n",
    "            'Full_Fp':  ['CONN', {'nll':True, 'kl_z':True, 'kl_p':True,  'Fa_norm':False, 'reg_loss':True,  'anneal':True}],\n",
    "            'Full_Fa':  ['CONN', {'nll':True, 'kl_z':True, 'kl_p':False, 'Fa_norm':False, 'reg_loss':False, 'anneal':True}],\n",
    "            'Fa':       ['SONN', {'nll':True, 'kl_z':True, 'kl_p':False, 'Fa_norm':False, 'reg_loss':False, 'anneal':True}],\n",
    "           }\n",
    "\n",
    "t = torch.arange(window_size + gamma + 1, dtype=dtype)/7\n",
    "eval_pts = np.arange(0,t.shape[-1], 7)\n",
    "epochs = 250\n",
    "q_sizes = [256, 128]\n",
    "ff_sizes = [128, 64]\n",
    "n_qs = 5\n",
    "latent_dim = 6\n",
    "epochs = 250\n",
    "region = 'state'\n",
    "started_file_path = \"HHS_started.txt\"\n",
    "ode_name = 'UONN'\n",
    "for num in range(10, 16):\n",
    "    for test_season in [2015, 2016, 2017, 2018]:\n",
    "        for key in list(mydict.keys()):\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 1\n",
    "test_season = 2015\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_data = DataConstructor(test_season=test_season, region = region, window_size=window_size, n_queries=n_qs, fill_1 = True, gamma=gamma)\n",
    "x_train, y_train, x_test, y_test, scaler = _data(run_backward=True, no_qs_in_output=True)\n",
    "train_loader, x_test, y_test = convert_to_torch(x_train, y_train, x_test, y_test, batch_size=32, shuffle=True, dtype=dtype) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 55/55 [00:03<00:00, 15.99it/s, Epoch=1, KL_z=1.4e+5] \n",
      "Training: 100%|██████████| 55/55 [00:03<00:00, 16.30it/s, Epoch=2, KL_z=4.83]\n",
      "Training: 100%|██████████| 55/55 [00:03<00:00, 16.27it/s, Epoch=3, KL_z=4.95]\n"
     ]
    }
   ],
   "source": [
    "ode_name = 'SONN'\n",
    "losses = mydict[key][1]\n",
    "ode = {'CONN':Fp, 'UONN':FaFp, 'SONN':Fa}[ode_name]\n",
    "num = 'test'\n",
    "file_prefix = f'weights/{ode_name}_{key}/{region}_{num}_{test_season}_'\n",
    "model = VAE(Encoder_Back_GRU, ode, Decoder, n_qs, latent_dim, n_regions, q_sizes=q_sizes, ff_sizes=ff_sizes, file_prefix=file_prefix)\n",
    "model.setup_training(lr=lr)\n",
    "\n",
    "model.pre_train(train_loader, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39marange(window_size \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m7\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 12\n",
    "test_season = 2015\n",
    "\n",
    "key = 'Fa'\n",
    "\n",
    "t = torch.arange(window_size + gamma + 1, dtype=dtype)/7\n",
    "eval_pts = torch.arange(57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_lr = 1e-3\n",
    "for param_group in model.optimizer.param_groups:\n",
    "    param_group['lr'] = new_lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training 1: 100%|██████████| 55/55 [01:46<00:00,  1.93s/it, loss=0.101, kl_w=0, nll=0.0996, kl_latent=0.00115, grad_norm=443]   \n",
      "Training 2: 100%|██████████| 55/55 [01:44<00:00,  1.89s/it, loss=-.0668, kl_w=0.000709, nll=-.0674, kl_latent=0.000691, grad_norm=0.655]\n",
      "Training 3: 100%|██████████| 55/55 [01:42<00:00,  1.86s/it, loss=-.0746, kl_w=0.00187, nll=-.0754, kl_latent=0.001, grad_norm=0.631]\n",
      "Training 4: 100%|██████████| 55/55 [01:42<00:00,  1.87s/it, loss=-.0761, kl_w=0.00371, nll=-.0774, kl_latent=0.001, grad_norm=0.669]\n",
      "Training 5: 100%|██████████| 55/55 [01:47<00:00,  1.96s/it, loss=-.0763, kl_w=0.00605, nll=-.0776, kl_latent=0.00105, grad_norm=0.685]\n",
      "Training 6: 100%|██████████| 55/55 [01:46<00:00,  1.94s/it, loss=-.0782, kl_w=0.00909, nll=-.0795, kl_latent=0.00131, grad_norm=0.589]\n",
      "Training 7: 100%|██████████| 55/55 [01:48<00:00,  1.97s/it, loss=-.0786, kl_w=0.0126, nll=-.08, kl_latent=0.00131, grad_norm=0.625]  \n",
      "Training 8: 100%|██████████| 55/55 [01:49<00:00,  1.98s/it, loss=-.0797, kl_w=0.0168, nll=-.0811, kl_latent=0.00133, grad_norm=0.571]\n",
      "Training 9: 100%|██████████| 55/55 [01:47<00:00,  1.95s/it, loss=-.0783, kl_w=0.0215, nll=-.0797, kl_latent=0.00116, grad_norm=0.664]\n",
      "Training 10: 100%|██████████| 55/55 [01:48<00:00,  1.97s/it, loss=-.0811, kl_w=0.0268, nll=-.0825, kl_latent=0.00131, grad_norm=0.533]\n",
      "Training 11: 100%|██████████| 55/55 [01:48<00:00,  1.97s/it, loss=-.0806, kl_w=0.0327, nll=-.082, kl_latent=0.00149, grad_norm=0.736] \n",
      "Training 12: 100%|██████████| 55/55 [01:48<00:00,  1.97s/it, loss=-.0815, kl_w=0.0391, nll=-.0825, kl_latent=0.001, grad_norm=0.742]\n",
      "Training 13: 100%|██████████| 55/55 [01:48<00:00,  1.97s/it, loss=-.0873, kl_w=0.046, nll=-.0885, kl_latent=0.001, grad_norm=0.818] \n",
      "Training 14: 100%|██████████| 55/55 [01:48<00:00,  1.97s/it, loss=-.107, kl_w=0.0535, nll=-.108, kl_latent=0.00155, grad_norm=1.57] \n",
      "Training 15: 100%|██████████| 55/55 [01:48<00:00,  1.98s/it, loss=-.131, kl_w=0.0616, nll=-.133, kl_latent=0.00175, grad_norm=1.65]\n",
      "Training 16: 100%|██████████| 55/55 [01:47<00:00,  1.96s/it, loss=-.142, kl_w=0.0701, nll=-.143, kl_latent=0.00198, grad_norm=2.14]\n",
      "Training 17: 100%|██████████| 55/55 [01:49<00:00,  1.99s/it, loss=-.153, kl_w=0.0792, nll=-.155, kl_latent=0.00225, grad_norm=1.71]\n",
      "Training 18: 100%|██████████| 55/55 [01:48<00:00,  1.98s/it, loss=-.157, kl_w=0.0888, nll=-.16, kl_latent=0.00262, grad_norm=1.91] \n",
      "Training 19: 100%|██████████| 55/55 [01:48<00:00,  1.98s/it, loss=-.158, kl_w=0.0989, nll=-.16, kl_latent=0.00251, grad_norm=2.43] \n",
      "Training 20: 100%|██████████| 55/55 [01:49<00:00,  1.99s/it, loss=-.167, kl_w=0.109, nll=-.169, kl_latent=0.00285, grad_norm=1.97]\n",
      "Training 21: 100%|██████████| 55/55 [01:47<00:00,  1.96s/it, loss=-.167, kl_w=0.12, nll=-.169, kl_latent=0.0024, grad_norm=2.39]  \n",
      "Training 22: 100%|██████████| 55/55 [01:48<00:00,  1.98s/it, loss=-.173, kl_w=0.132, nll=-.175, kl_latent=0.00271, grad_norm=2.32]\n",
      "Training 23: 100%|██████████| 55/55 [01:48<00:00,  1.98s/it, loss=-.176, kl_w=0.144, nll=-.179, kl_latent=0.00275, grad_norm=2.22]\n",
      "Training 24: 100%|██████████| 55/55 [01:49<00:00,  1.98s/it, loss=-.181, kl_w=0.156, nll=-.184, kl_latent=0.00291, grad_norm=1.99]\n",
      "Training 25: 100%|██████████| 55/55 [01:49<00:00,  1.99s/it, loss=-.182, kl_w=0.169, nll=-.184, kl_latent=0.0028, grad_norm=2.24] \n",
      "Training 26: 100%|██████████| 55/55 [01:48<00:00,  1.97s/it, loss=-.184, kl_w=0.182, nll=-.187, kl_latent=0.00291, grad_norm=2.12]\n",
      "Training 27: 100%|██████████| 55/55 [01:50<00:00,  2.01s/it, loss=-.185, kl_w=0.196, nll=-.188, kl_latent=0.00287, grad_norm=2.38]\n",
      "Training 28: 100%|██████████| 55/55 [01:49<00:00,  2.00s/it, loss=-.186, kl_w=0.209, nll=-.189, kl_latent=0.00276, grad_norm=2.24]\n",
      "Training 29: 100%|██████████| 55/55 [01:49<00:00,  1.98s/it, loss=-.187, kl_w=0.224, nll=-.19, kl_latent=0.00285, grad_norm=2.62] \n",
      "Training 30: 100%|██████████| 55/55 [01:51<00:00,  2.03s/it, loss=-.191, kl_w=0.238, nll=-.194, kl_latent=0.00293, grad_norm=2.27]\n",
      "Training 31: 100%|██████████| 55/55 [01:51<00:00,  2.03s/it, loss=-.193, kl_w=0.253, nll=-.196, kl_latent=0.00313, grad_norm=2.07]\n",
      "Training 32: 100%|██████████| 55/55 [01:51<00:00,  2.03s/it, loss=-.191, kl_w=0.268, nll=-.194, kl_latent=0.00287, grad_norm=2.41]\n",
      "Training 33: 100%|██████████| 55/55 [01:50<00:00,  2.01s/it, loss=-.192, kl_w=0.284, nll=-.195, kl_latent=0.0028, grad_norm=2.35] \n",
      "Training 34: 100%|██████████| 55/55 [01:51<00:00,  2.03s/it, loss=-.192, kl_w=0.299, nll=-.195, kl_latent=0.00295, grad_norm=2.59]\n",
      "Training 35: 100%|██████████| 55/55 [01:52<00:00,  2.05s/it, loss=-.195, kl_w=0.315, nll=-.198, kl_latent=0.00304, grad_norm=1.93]\n",
      "Training 36: 100%|██████████| 55/55 [01:49<00:00,  2.00s/it, loss=-.197, kl_w=0.332, nll=-.2, kl_latent=0.00318, grad_norm=1.97]  \n",
      "Training 37: 100%|██████████| 55/55 [01:52<00:00,  2.04s/it, loss=-.195, kl_w=0.348, nll=-.198, kl_latent=0.003, grad_norm=2.41]  \n",
      "Training 38: 100%|██████████| 55/55 [01:51<00:00,  2.03s/it, loss=-.195, kl_w=0.364, nll=-.198, kl_latent=0.00282, grad_norm=2.44]\n",
      "Training 39: 100%|██████████| 55/55 [01:52<00:00,  2.04s/it, loss=-.2, kl_w=0.381, nll=-.203, kl_latent=0.00316, grad_norm=1.7]   \n",
      "Training 40: 100%|██████████| 55/55 [01:53<00:00,  2.06s/it, loss=-.2, kl_w=0.398, nll=-.203, kl_latent=0.00305, grad_norm=1.79]  \n",
      "Training 41: 100%|██████████| 55/55 [01:51<00:00,  2.03s/it, loss=-.2, kl_w=0.415, nll=-.204, kl_latent=0.00316, grad_norm=2.05]  \n",
      "Training 42: 100%|██████████| 55/55 [01:54<00:00,  2.07s/it, loss=-.201, kl_w=0.432, nll=-.204, kl_latent=0.00318, grad_norm=1.7] \n",
      "Training 43: 100%|██████████| 55/55 [01:53<00:00,  2.07s/it, loss=-.201, kl_w=0.449, nll=-.204, kl_latent=0.00311, grad_norm=1.83]\n",
      "Training 44: 100%|██████████| 55/55 [01:51<00:00,  2.04s/it, loss=-.201, kl_w=0.466, nll=-.204, kl_latent=0.00302, grad_norm=2.12]\n",
      "Training 45: 100%|██████████| 55/55 [01:51<00:00,  2.03s/it, loss=-.2, kl_w=0.484, nll=-.203, kl_latent=0.00296, grad_norm=2.14]  \n",
      "Training 46: 100%|██████████| 55/55 [01:54<00:00,  2.07s/it, loss=-.202, kl_w=0.501, nll=-.205, kl_latent=0.00311, grad_norm=2.03]\n",
      "Training 47: 100%|██████████| 55/55 [01:52<00:00,  2.04s/it, loss=-.2, kl_w=0.518, nll=-.203, kl_latent=0.00293, grad_norm=2.41]  \n",
      "Training 48: 100%|██████████| 55/55 [01:52<00:00,  2.04s/it, loss=-.203, kl_w=0.535, nll=-.206, kl_latent=0.00311, grad_norm=2.02]\n",
      "Training 49: 100%|██████████| 55/55 [01:54<00:00,  2.08s/it, loss=-.205, kl_w=0.553, nll=-.208, kl_latent=0.00316, grad_norm=1.79]\n",
      "Training 50: 100%|██████████| 55/55 [01:52<00:00,  2.04s/it, loss=-.204, kl_w=0.57, nll=-.207, kl_latent=0.00304, grad_norm=1.95] \n",
      "Training 51: 100%|██████████| 55/55 [01:53<00:00,  2.06s/it, loss=-.205, kl_w=0.587, nll=-.208, kl_latent=0.00311, grad_norm=1.95]\n",
      "Training 52: 100%|██████████| 55/55 [01:52<00:00,  2.05s/it, loss=-.204, kl_w=0.604, nll=-.207, kl_latent=0.00289, grad_norm=2.32]\n",
      "Training 53: 100%|██████████| 55/55 [01:52<00:00,  2.05s/it, loss=-.206, kl_w=0.621, nll=-.209, kl_latent=0.00305, grad_norm=1.79]\n",
      "Training 54: 100%|██████████| 55/55 [01:53<00:00,  2.06s/it, loss=-.207, kl_w=0.637, nll=-.21, kl_latent=0.00302, grad_norm=1.86] \n",
      "Training 55: 100%|██████████| 55/55 [01:52<00:00,  2.04s/it, loss=-.205, kl_w=0.654, nll=-.208, kl_latent=0.00289, grad_norm=2.33]\n",
      "Training 56: 100%|██████████| 55/55 [01:53<00:00,  2.06s/it, loss=-.208, kl_w=0.67, nll=-.211, kl_latent=0.00302, grad_norm=2.06] \n",
      "Training 57: 100%|██████████| 55/55 [01:54<00:00,  2.08s/it, loss=-.208, kl_w=0.686, nll=-.211, kl_latent=0.00289, grad_norm=1.88]\n",
      "Training 58: 100%|██████████| 55/55 [01:51<00:00,  2.03s/it, loss=-.209, kl_w=0.702, nll=-.212, kl_latent=0.00295, grad_norm=1.99]\n",
      "Training 59: 100%|██████████| 55/55 [01:52<00:00,  2.05s/it, loss=-.209, kl_w=0.718, nll=-.212, kl_latent=0.00298, grad_norm=1.91]\n",
      "Training 60: 100%|██████████| 55/55 [01:52<00:00,  2.05s/it, loss=-.21, kl_w=0.733, nll=-.213, kl_latent=0.0028, grad_norm=2]     \n",
      "Training 61: 100%|██████████| 55/55 [01:53<00:00,  2.06s/it, loss=-.211, kl_w=0.749, nll=-.214, kl_latent=0.00289, grad_norm=1.65]\n",
      "Training 62: 100%|██████████| 55/55 [01:53<00:00,  2.06s/it, loss=-.21, kl_w=0.763, nll=-.213, kl_latent=0.00273, grad_norm=2.04] \n",
      "Training 63: 100%|██████████| 55/55 [01:53<00:00,  2.07s/it, loss=-.212, kl_w=0.778, nll=-.214, kl_latent=0.00284, grad_norm=1.74]\n",
      "Training 64: 100%|██████████| 55/55 [01:52<00:00,  2.05s/it, loss=-.212, kl_w=0.792, nll=-.215, kl_latent=0.00282, grad_norm=1.98]\n",
      "Training 65: 100%|██████████| 55/55 [01:52<00:00,  2.04s/it, loss=-.213, kl_w=0.806, nll=-.216, kl_latent=0.00262, grad_norm=1.88]\n",
      "Training 66: 100%|██████████| 55/55 [01:52<00:00,  2.04s/it, loss=-.213, kl_w=0.819, nll=-.215, kl_latent=0.00258, grad_norm=1.85]\n",
      "Training 67: 100%|██████████| 55/55 [01:54<00:00,  2.09s/it, loss=-.215, kl_w=0.833, nll=-.218, kl_latent=0.0028, grad_norm=1.7]  \n",
      "Training 68: 100%|██████████| 55/55 [01:53<00:00,  2.06s/it, loss=-.213, kl_w=0.845, nll=-.216, kl_latent=0.00247, grad_norm=2.06]\n",
      "Training 69: 100%|██████████| 55/55 [01:52<00:00,  2.04s/it, loss=-.213, kl_w=0.858, nll=-.215, kl_latent=0.00245, grad_norm=2.44]\n",
      "Training 70: 100%|██████████| 55/55 [01:51<00:00,  2.03s/it, loss=-.215, kl_w=0.869, nll=-.217, kl_latent=0.00249, grad_norm=1.78]\n",
      "Training 71: 100%|██████████| 55/55 [01:53<00:00,  2.06s/it, loss=-.217, kl_w=0.881, nll=-.219, kl_latent=0.00255, grad_norm=1.8] \n",
      "Training 72: 100%|██████████| 55/55 [01:53<00:00,  2.06s/it, loss=-.218, kl_w=0.892, nll=-.22, kl_latent=0.00262, grad_norm=1.82] \n",
      "Training 73: 100%|██████████| 55/55 [01:54<00:00,  2.08s/it, loss=-.215, kl_w=0.902, nll=-.217, kl_latent=0.00229, grad_norm=2.09]\n",
      "Training 74: 100%|██████████| 55/55 [01:52<00:00,  2.04s/it, loss=-.22, kl_w=0.912, nll=-.222, kl_latent=0.00253, grad_norm=1.63] \n",
      "Training 75: 100%|██████████| 55/55 [01:54<00:00,  2.08s/it, loss=-.22, kl_w=0.922, nll=-.223, kl_latent=0.00251, grad_norm=1.75] \n",
      "Training 76: 100%|██████████| 55/55 [01:53<00:00,  2.07s/it, loss=-.222, kl_w=0.931, nll=-.224, kl_latent=0.00245, grad_norm=1.61]\n",
      "Training 77: 100%|██████████| 55/55 [01:53<00:00,  2.07s/it, loss=-.223, kl_w=0.939, nll=-.225, kl_latent=0.00249, grad_norm=1.56]\n",
      "Training 78: 100%|██████████| 55/55 [01:54<00:00,  2.07s/it, loss=-.221, kl_w=0.947, nll=-.223, kl_latent=0.00236, grad_norm=2.19]\n",
      "Training 79: 100%|██████████| 55/55 [01:52<00:00,  2.05s/it, loss=-.221, kl_w=0.955, nll=-.223, kl_latent=0.00227, grad_norm=1.99]\n",
      "Training 80: 100%|██████████| 55/55 [01:50<00:00,  2.01s/it, loss=-.226, kl_w=0.962, nll=-.228, kl_latent=0.0024, grad_norm=1.53] \n",
      "Training 81:   7%|▋         | 4/55 [00:07<01:38,  1.94s/it, loss=-.224, kl_w=0.965, nll=-.227, kl_latent=0.0025, grad_norm=1.47] "
     ]
    }
   ],
   "source": [
    "model.train(train_loader, t, epochs, losses, eval_pts, grad_lim=650, checkpoint=True, track_norms=True, norm_file=f'{file_prefix}norms.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(train_loader, t, epochs, losses, eval_pts, grad_lim=300, n_samples=32, checkpoint=False, track_norms = False, norm_file = 'grad_norms.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for test_season in [2015, 2016, 2017, 2018]:\n",
    "        _data = DataConstructor(test_season=test_season, region = region, window_size=window_size, n_queries=n_qs, gamma=gamma)\n",
    "        x_train, y_train, x_test, y_test, scaler = _data(run_backward=True, no_qs_in_output=True)\n",
    "        train_loader, x_test, y_test = convert_to_torch(x_train, y_train, x_test, y_test, batch_size=32, shuffle=True, dtype=dtype) \n",
    "                \n",
    "        for num in range(10, 16):\n",
    "            for key in list(mydict.keys()):\n",
    "                try:\n",
    "                    file_prefix = f'weights/{ode_name}_{key}/{region}_{num}_{test_season}_'\n",
    "                    ode_name = mydict[key][0]\n",
    "                    losses = mydict[key][1]\n",
    "                    ode = {'CONN':Fp, 'UONN':FaFp, 'SONN':Fa}[ode_name]\n",
    "\n",
    "                    model = VAE(Encoder_Back_GRU, ode, Decoder, n_qs, latent_dim, n_regions, q_sizes=q_sizes, ff_sizes=ff_sizes, file_prefix=file_prefix)\n",
    "                    model.load(checkpoint=True)\n",
    "\n",
    "                    y_pred = model(x_test, t, n_samples=128)\n",
    "                    y_pr = y_pred.detach().numpy() * scaler.values[np.newaxis, np.newaxis, np.newaxis, :]\n",
    "                    y_te = y_test.detach().numpy() * scaler.values[np.newaxis, np.newaxis, :]\n",
    "\n",
    "                    pred_mean = y_pr.mean(1)\n",
    "                    pred_std = y_pr.std(1)\n",
    "\n",
    "                    results = {'ground_truth':y_te.tolist(),\n",
    "                            'mean':pred_mean.tolist(),\n",
    "                            'std': pred_std.tolist(),\n",
    "                            }\n",
    "\n",
    "                    skills = pd.DataFrame(index = [7,14,21,28], columns = [2015,2016,2017,2018], dtype=float)\n",
    "                    for g_tr, g in zip([7,14,21,28], [-22,-15,-8,-1]):\n",
    "                        skills.loc[g_tr, test_season] = np.exp(np.mean(Metrics.mb_log(y_te[:, g, :], pred_mean[:, g, :], pred_std[:, g, :]))).astype(float)\n",
    "                    print(file_prefix)\n",
    "                    print(np.exp(np.mean(np.log(skills), 1)))\n",
    "                except:\n",
    "                    print(file_prefix)\n",
    "                    print('failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# typo where Full_Fa is actually Half_Fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 1\n",
    "\n",
    "\n",
    "q_sizes = [128, 64,32]\n",
    "ff_sizes = [64]\n",
    "n_qs = 10\n",
    "latent_dim = 8\n",
    "epochs = 100\n",
    "region = 'state'\n",
    "for num in range(2,6):\n",
    "    for test_season in [2015, 2016, 2017, 2018]:\n",
    "        for ode_name in ['UONN']:\n",
    "            file_prefix = f'weights/{ode_name}/{region}_{num}_{test_season}_'\n",
    "                    \n",
    "            print(ode_name, test_season, num)\n",
    "            ode = {'CONN':Fp, 'UONN':FaFp, 'SONN':Fa}[ode_name]\n",
    "\n",
    "            _data = DataConstructor(test_season=test_season, region = region, window_size=window_size, n_queries=n_qs, gamma=gamma)\n",
    "            x_train, y_train, x_test, y_test, scaler = _data(run_backward=True, no_qs_in_output=True)\n",
    "            train_loader, x_test, y_test = convert_to_torch(x_train, y_train, x_test, y_test, batch_size=32, shuffle=True, dtype=dtype)  \n",
    "\n",
    "            model = VAE(Encoder_Back_GRU, ode, Decoder, n_qs, latent_dim, n_regions, q_sizes=q_sizes, ff_sizes=ff_sizes, file_prefix=file_prefix)\n",
    "            model.setup_training(lr=lr)\n",
    "\n",
    "            model.pre_train(train_loader, 3)\n",
    "            model.train(train_loader, t, epochs, eval_pts, grad_lim=650, checkpoint=True, track_norms=True, norm_file=f'{file_prefix}norms.txt')\n",
    "            model.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 2\n",
    "test_season = 2018\n",
    "ode_name = 'UONN'\n",
    "\n",
    "file_prefix = f'weights/{ode_name}/{region}_{num}_{test_season}_'\n",
    "        \n",
    "print(ode_name, test_season, num)\n",
    "ode = {'CONN':Fp, 'UONN':FaFp, 'SONN':Fa}[ode_name]\n",
    "\n",
    "_data = DataConstructor(test_season=test_season, region = region, window_size=window_size, n_queries=n_qs, gamma=gamma)\n",
    "x_train, y_train, x_test, y_test, scaler = _data(run_backward=True, no_qs_in_output=True)\n",
    "train_loader, x_test, y_test = convert_to_torch(x_train, y_train, x_test, y_test, batch_size=32, shuffle=True, dtype=dtype)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(Encoder_Back_GRU, ode, Decoder, n_qs, latent_dim, n_regions, q_sizes=q_sizes, ff_sizes=ff_sizes, file_prefix=file_prefix)\n",
    "model.setup_training(lr=lr)\n",
    "\n",
    "model.pre_train(train_loader, 3)\n",
    "model.train(train_loader, t, epochs, eval_pts, grad_lim=650, checkpoint=True, track_norms=True, norm_file=f'{file_prefix}norms.txt')\n",
    "# model.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 1\n",
    "\n",
    "\n",
    "q_sizes = [128, 64,32]\n",
    "ff_sizes = [64]\n",
    "n_qs = 10\n",
    "latent_dim = 8\n",
    "epochs = 100\n",
    "region = 'state'\n",
    "for num in range(2,3):\n",
    "    for test_season in [2016]:\n",
    "        for ode_name in ['CONN']:\n",
    "            \n",
    "\n",
    "            file_prefix = f'weights/{ode_name}/{region}_{num}_{test_season}_'\n",
    "            run = False\n",
    "                    \n",
    "            print(ode_name, test_season, num)\n",
    "            ode = {'CONN':Fp, 'UONN':FaFp, 'SONN':Fa}[ode_name]\n",
    "\n",
    "            _data = DataConstructor(test_season=test_season, region = region, window_size=window_size, n_queries=n_qs, gamma=gamma)\n",
    "            x_train, y_train, x_test, y_test, scaler = _data(run_backward=True, no_qs_in_output=True)\n",
    "            train_loader, x_test, y_test = convert_to_torch(x_train, y_train, x_test, y_test, batch_size=32, shuffle=True, dtype=dtype)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(Encoder_Back_GRU, ode, Decoder, n_qs, latent_dim, n_regions, q_sizes=q_sizes, ff_sizes=ff_sizes, file_prefix=file_prefix)\n",
    "model.load()\n",
    "\n",
    "y_pred = model(x_test, t, n_samples=128)\n",
    "y_pr = y_pred.detach().numpy() * scaler.values[np.newaxis, np.newaxis, np.newaxis, :]\n",
    "y_te = y_test.detach().numpy() * scaler.values[np.newaxis, np.newaxis, :]\n",
    "\n",
    "pred_mean = y_pr.mean(1)\n",
    "pred_std = y_pr.std(1)\n",
    "\n",
    "results = {'ground_truth':y_te.tolist(),\n",
    "        'mean':pred_mean.tolist(),\n",
    "        'std': pred_std.tolist(),\n",
    "        }\n",
    "\n",
    "skills = pd.DataFrame(index = [7,14,21,28], columns = [2015,2016,2017,2018], dtype=float)\n",
    "for g_tr, g in zip([7,14,21,28], [-22,-15,-8,-1]):\n",
    "    skills.loc[g_tr, test_season] = np.exp(np.mean(Metrics.mb_log(y_te[:, g, :], pred_mean[:, g, :], pred_std[:, g, :]))).astype(float)\n",
    "print(np.exp(np.mean(np.log(skills), 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_codes = {'AK':'Alaska','AL':'Alabama','AR':'Arkansas','AZ':'Arizona','CA':'California','CO':'Colorado',\n",
    "                'CT':'Connecticut','DE':'Delaware','DC':'District of Columbia','GA':'Georgia', 'HI':'Hawaii',\n",
    "                'ID':'Idaho','IL':'Illinois','IN':'Indiana','IA':'Iowa','KS':'Kansas','KY':'Kentucky',\n",
    "                'LA':'Louisiana','ME':'Maine','MD':'Maryland','MI':'Michigan','MN':'Minnesota','MS':'Mississippi',\n",
    "                'MO':'Missouri','MT':'Montana','NE':'Nebraska','NV':'Nevada','NH':'New Hampshire','NJ':'New Jersey',\n",
    "                'NM':'New Mexico','NY':'New York','NC':'North Carolina','ND':'North Dakota','OH':'Ohio','OK':'Oklahoma',\n",
    "                'OR':'Oregon','PA':'Pennsylvania','RI':'Rhode Island','SC':'South Carolina','SD':'South Dakota',\n",
    "                'TN':'Tennessee','TX':'Texas','UT':'Utah','VT':'Vermont','VA':'Virginia','WA':'Washington',\n",
    "                'WV':'West Virginia','WI':'Wisconsin','WY':'Wyoming'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = 'CA'\n",
    "state_num = np.where([code == state_code for state_code in list(state_codes.keys())])[0][0]\n",
    "\n",
    "plt.plot(np.asarray(results['ground_truth'])[:, -1, state_num], color='black')\n",
    "plt.plot(np.asarray(results['mean'])[:, -21, state_num], color='red')\n",
    "plt.fill_between(np.arange(np.asarray(results['mean']).shape[0]),\n",
    "                 (np.asarray(results['mean'])-np.asarray(results['std']))[:, -21, state_num],\n",
    "                 (np.asarray(results['mean'])+np.asarray(results['std']))[:, -21, state_num], \n",
    "                 color='red', linewidth=0, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(np.asarray(results['mean']).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def function(queries, N):\n",
    "    array = np.zeros(N)\n",
    "    return_list = []\n",
    "    for inputs in queries:\n",
    "        setget = inputs[0]\n",
    "\n",
    "        if setget == 1:\n",
    "            array[inputs[1]] = 1\n",
    "\n",
    "        if setget == 2:\n",
    "            pos = inputs[1]\n",
    "\n",
    "            try:\n",
    "                return_list.append(np.where(array[pos:] == 1)[0][0] + pos)\n",
    "            except:\n",
    "                return_list.append(-1)\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [[2, 3], [1, 2], [2, 1], [2, 3], [2, 2]]\n",
    "N = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, 2, -1, 2]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function(queries, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def function2(A, N):\n",
    "    return_arr = []\n",
    "    for l1 in range(N):\n",
    "        for r1 in range(l1+1, N+1):\n",
    "            sub = np.mean(A[l1:r1])\n",
    "            \n",
    "            sub2r = np.sum(A[r1:])\n",
    "            sub2l = np.sum(A[:l1])\n",
    "            \n",
    "            if np.isnan(sub2r):\n",
    "                sub2r=0\n",
    "                \n",
    "            if np.isnan(sub2l):\n",
    "                sub2l=0\n",
    "            if len(A[r1:])+len(A[:l1]) == 0:\n",
    "                sub2 = 0\n",
    "            else:\n",
    "                sub2 = (sub2l+sub2r)/(len(A[r1:])+len(A[:l1]))\n",
    "            if sub > sub2:\n",
    "                return_arr.append([l1+1, r1])\n",
    "    return return_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [3, 4, 2]\n",
    "N = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2], [1, 3], [2, 2]]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function2(A, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Torch)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
